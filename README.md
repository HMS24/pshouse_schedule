# pshouse schedule
æ“·å–é å”®æ¡ˆå¯¦åƒ¹ç™»éŒ„çš„æ’ç¨‹ pre-sale house schedule

## æè¿°

é—œæ–¼**é å”®å±‹å¯¦åƒ¹ç™»éŒ„**çš„ EtL å¯¦ä½œï¼Œæ–¼æ¯æœˆ 1 è™Ÿã€11 è™ŸåŠ 21 è™Ÿå¾å…§æ”¿éƒ¨ç¶²ç«™ä¸‹è¼‰é å”®å±‹å¯¦åƒ¹ç™»éŒ„ï¼Œä¸¦æª¢æŸ¥è³‡æ–™æ˜¯å¦å·²å‚™ä»½åŠæ›´æ–° databaseã€‚
é™¤äº†æ–°å¢æ­·å²è³‡æ–™ä»¥å¤–ï¼Œä¹Ÿèƒ½æ›´æ–°æ·¡æ°´å€ 2022 å¹´æŒ‰æœˆåŠæŒ‰é å”®æ¡ˆåˆ†é¡çš„å¹³å‡ç¸½åƒ¹ã€å¹³å‡æˆ¿åƒ¹åŠå¹³å‡å–®åƒ¹çµ±è¨ˆè¡¨è³‡æ–™ã€‚

é¦–å…ˆæ“·å–è³‡æ–™ä¸¦ä¸Šå‚³ AWS S3 å‚™ä»½ã€‚å…¶æ¬¡åˆ©ç”¨ `pandas` åšè³‡æ–™çš„å‰è™•ç†ï¼Œä¸¦æ’å…¥è³‡æ–™åˆ° ï¼­ySql è½åœ°æ‡‰ç”¨ã€‚æœ€å¾Œå„²å­˜ validate æœ‰å•é¡Œçš„å¯¦åƒ¹ç™»éŒ„ï¼Œç”±äººå·¥æ ¡æ­£ã€‚

æ’ç¨‹å‰‡ä»¥ `APScheduler` å¯¦ä½œã€‚å¦å¤–ç•¶æ“·å–çš„æµç¨‹çµæŸå¾Œï¼Œæœƒç¢ºèªè³‡æ–™æ˜¯å¦æœ‰å‚™ä»½åŠè¼‰å…¥ databaseï¼Œè‹¥ç„¡å‰‡é–“éš” 1 å°æ™‚å†åšä¸€æ¬¡æ“·å–ç¨‹åºã€‚éƒ¨ç½²æ–¹å¼ä»¥ container éƒ¨ç½²åˆ° AWS EC2 ä¸ŠåŸ·è¡Œã€‚

**é™åˆ¶ 1: ç›®å‰åƒ…æ“·å–æ–°åŒ—å¸‚çš„ç•¶æœŸé å”®è³‡æ–™ã€‚æ­·å²è³‡æ–™å·²æ”¾ `results` è³‡æ–™å¤¾ã€‚å€é–“åŒ…å« 2021 ç¬¬ 2 å­£è‡³ 20220911(æœ€æ–°ä¸€æœŸ)ã€‚**

**é™åˆ¶ 2: å°ˆæ¡ˆèˆ‡ [pshouse](https://github.com/HMS24/pshouse) web application åˆä½œï¼ŒåˆæœŸå…±ç”¨ database ç”± web application è² è²¬ migrate databaseã€‚ä¸¦ä¸”ä¹Ÿä»¥ container çš„æ–¹å¼é‹è¡Œã€‚è€Œç‚ºæ–¹ä¾¿ 2 é‚Šçš„ `compose.yml` æºé€šï¼Œåœ¨ deploy éšæ®µæœƒæ–°å¢ docker network backend_net å¦‚ä¸‹**

- [pshouse/compose.yml](https://github.com/HMS24/pshouse/blob/master/compose.yml#L46)
- [pshouse schedule/compose.yml](https://github.com/HMS24/pshouse_schedule/blob/master/compose.yml#L18)

ä¹‹å¾Œå°‡ database æœå‹™ç¨ç«‹å‡ºä¾†ï¼Œä¾‹å¦‚ä½¿ç”¨ AWS rdsï¼Œå°±å¯ä»¥åˆªé™¤é€™æ®µ code.ğŸ¥²
- [run.sh](https://github.com/HMS24/pshouse_schedule/blob/master/run.sh#L79)
- [deploy/publish.sh](https://github.com/HMS24/pshouse_schedule/blob/master/deploy/publish.sh#L20)
    
## å¦‚ä½•ä½¿ç”¨
### é–‹ç™¼

    $ pip3 install -r requirements.txt
    $ python3 app.py

### éƒ¨ç½²å‰ç½®ä½œæ¥­

è¦éƒ¨ç½²åˆ°é ç«¯æ©Ÿå™¨ï¼Œå‡è¨­ç›®æ¨™æ©Ÿå™¨ OS ç‚º `Ubuntu 20.04`:
1. å®‰è£ `docker` and `docker compose`
2. æ–°å¢è³‡æ–™å¤¾ `mkdir ~/pshs` ([`./deploy/publish.sh`](https://github.com/HMS24/pshouse_schedule/blob/master/deploy/publish.sh#L16) æœ‰å¯«å…¥è³‡æ–™å¤¾çš„åç¨± )
3. è¨­ç½®ç’°å¢ƒè®Šæ•¸ `cd ~/pshs && vi .env`
    - `DATABASE_URI` é è¨­ sqlite
    - `SQL_ECHO` [True|False] log sql statements? é è¨­ false
    - `STORAGE_TYPE=S3` [S3|LOCAL] ä¸Šå‚³è‡³å“ªè£¡ï¼Ÿ
    - `STORAGE_KEY` AWS çš„ key
    - `STORAGE_SECRET` AWS çš„ secret
    - `TZ` container æ™‚å€ï¼Œé è¨­ `Asia/Taipei`

### æœ¬åœ°éƒ¨ç½²
    
è¨­ç½®ç’°å¢ƒè®Šæ•¸

    $ cp .env.example .env

å»ºç«‹æ˜ åƒæª”ä¸¦éƒ¨ç½²ã€‚é è¨­æ˜ åƒæª”åç¨±åŠç‰ˆæœ¬: `local/pshs:latest`

    $ ./run.sh --target local

æŸ¥çœ‹ log

    $ docker logs -f pshs
    
    2022-09-01 00:50:17 [INFO] Adding job tentatively -- it will be properly scheduled when the scheduler starts
    2022-09-01 00:50:17 [INFO] Added job "crawl_deals" to job store "default"
    2022-09-01 00:50:17 [INFO] Scheduler started

### é ç«¯éƒ¨ç½²

å»ºç«‹æ˜ åƒæª”ä¸Šå‚³ docker hub ä¸¦éƒ¨ç½²ï¼Œé è¨­æ˜ åƒæª”åç¨±:`$DOCKER_USER/$IMAGE:$TAG`

    $ ./run.sh --target $REMOTE_MACHINE \
               --ssh-pem $REMOTE_MACHINE_PEM_PATH \
               --docker-user $DOCKER_USER \
               --docker-pass $DOCKER_PASSWORD_PATH \
               --image $IMAGE \
               --tag $TAG \
               --init $INIT_DATABASE

Parameters
- `REMOTE_MACHINE`: é ç«¯æ©Ÿå™¨ (user@hostname)
- `REMOTE_MACHINE_PEM_PATH`: pem æª”æ¡ˆä½ç½® ("$HOME/***.pem")
- `DOCKER_USER`: docker ä½¿ç”¨è€…
- `DOCKER_PASSWORD_PATH` docker å¯†ç¢¼æª”æ¡ˆä½ç½® ("$HOME/***")
- `IMAGE`(optional): æ˜ åƒæª”åç¨±
- `TAG`(optional) æ˜ åƒæª” tag
- `INIT_DATABASE`(optional) 
é‡ç½® database [0|1] `1` æœƒ truncate tableï¼Œå†æ’å…¥ `results`æ”¾çš„æ­·å²è³‡æ–™ã€‚é è¨­ 0ã€‚
ä¹Ÿæœƒ**æ›´æ–°çµ±è¨ˆè¡¨è³‡æ–™(å°šæœªè‡ªå‹•æ›´æ–°)**

## æ¶æ§‹

```shell
.
â”œâ”€â”€ build
â”‚   â”œâ”€â”€ build.sh
â”‚   â”œâ”€â”€ test.sh                 # å•Ÿå‹•ä¸€å€‹ container è·‘æ¸¬è©¦
â”‚   â”œâ”€â”€ push.sh
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ deploy               
â”‚   â”œâ”€â”€ deploy.sh           
â”‚   â””â”€â”€ publish.sh              # åœ¨é ç«¯æ©Ÿå™¨éƒ¨ç½²çš„ script
â”œâ”€â”€ pshouse_schedule
â”‚   â”œâ”€â”€ db
â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â””â”€â”€ stores.py
â”‚   â”œâ”€â”€ jobs.py                 # å®šç¾© scheule çš„ job åŠ event handlers
â”‚   â”œâ”€â”€ processes.py            # æµç¨‹è™•ç†çš„ functions
â”‚   â”œâ”€â”€ fetch.py           
â”‚   â”œâ”€â”€ parse.py
â”‚   â”œâ”€â”€ load.py
â”‚   â”œâ”€â”€ storage.py
â”‚   â”œâ”€â”€ schemas.py              # validate åŠ type conversion
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ exceptions.py
â”œâ”€â”€ tests                       # æ¸¬è©¦                 
â”œâ”€â”€ results                     # å­˜æŠ“ä¸‹ä¾†çš„ csv æª”
â”œâ”€â”€ .env                    
â”œâ”€â”€ app.py                      # ç¨‹å¼å…¥å£
â”œâ”€â”€ boot.sh                     # container entrypoint script
â”œâ”€â”€ compose.yml
â””â”€â”€ run.sh                      # åŸ·è¡Œ build and deploy çš„ script
```

### modules é‚è¼¯é †åºï¼š
<p align="center">
<img src="./assets/module_order.jpeg" alt="scheduler" width="1080"/>
</p>

### modules èªªæ˜ï¼š

- `boot.sh`: æœªä¾†å¦‚è¦ä¿è­‰ container çš„å•Ÿå‹•é †åºï¼Œéœ€å¢åŠ  wait for db çš„ scriptï¼Œå¯ä»¥ä¸ç”¨å† build imageï¼Œä»¥ç¯€çœæ™‚é–“ã€‚
- `app.py`: start schedule and keeps the main thread alived.
- `jobs.py`: å®šç¾© cronjob åŠæ‰€èª¿ç”¨çš„ processã€‚åŒ…å« `APScheduler` é è¨­çš„äº‹ä»¶è™•ç† handlers
- `processes.py`: ä¸»è¦çš„æµç¨‹è™•ç† functionsã€‚
- `fetch.py`: æ“·å–è³‡æ–™çš„ functionsã€‚
- `parse.py`: è³‡æ–™æ“·å–å¾Œçš„è™•ç†æœƒè½‰æ›ä¸­è‹±æ¬„ä½ã€ç§»é™¤è‹±æ–‡æ¨™é¡Œåˆ—ã€æ—¥æœŸè½‰æ›è‡³è¥¿å…ƒæ—¥æœŸã€å¡«è£œå„æ¬„ç©ºå€¼åŠæ–°å¢ `city` æ¬„ä½ã€‚
- `load.py`: è³‡æ–™æ’å…¥ MySql è³‡æ–™åº«ã€‚
- `storage.py`: ç”¨ `cloudstorage` å¯¦ä½œçš„ storage ä»‹é¢ï¼Œå¯ä»¥æ ¹æ“š `.env` çš„ `STORAGE_TYPE` é¸æ“‡ä¸Šå‚³æœ¬åœ°æˆ–é›²ç«¯(S3)
- `schemas.py`: å­˜å…¥ database å‰å°‡æ¬„ä½è³‡æ–™è½‰å‹ä¸¦åšåŸºæœ¬çš„ validateã€‚
- db
    - `database.py`: æœ‰å¯¦ä½œ session çš„ context managerï¼Œä»¥ä¾¿çµ±ä¸€ç®¡ç† session çš„ rollback åŠ closeã€‚
    - `models.py`:  database table çš„æŠ½è±¡å±¤ã€‚
    - `stores.py`: æ“ä½œ database çš„é‚è¼¯å±¤ã€‚å› ç‚ºéœ€æ±‚ç°¡å–®ï¼Œæ‰€ä»¥ç›´æ¥åœ¨é€™å€‹ module æ“ä½œ model è€Œéç”¨ __init__ çš„æ–¹å¼å‚³é€²å»ã€‚
- build
    - `build.sh`: æŒ‡å®šç‰¹å®š platform (linux/amd64)ï¼Œèˆ‡è¦éƒ¨ç½²çš„é ç«¯æ©Ÿå™¨ OS ä¸€è‡´ã€‚
    - `test.sh`:  run container ä¸¦ä¸”å®‰è£ pytest è·‘æ¸¬è©¦
    - `push.sh`: ç™»å…¥ docker æ¨è‡³ hub
- deploy
    - `deploy.sh`: å°‡ compose.yml, .auth åŠ publish.sh å‚³è‡³é ç«¯æ©Ÿå™¨æº–å‚™éƒ¨ç½²
    - `publish.sh`: éƒ¨ç½² script

### æµç¨‹åœ–

<p align="center">
<img src="./assets/all.jpg" alt="all" width="1680"/>
</p>

### æµç¨‹èªªæ˜

- scheduler æ–°å¢ crawl cronjob
- æ¯æœˆ 1, 11, 21 æ—©ä¸Š 10:00 è§¸ç™¼è©² jobã€‚
- ä»¥ 9/21 ç‚ºä¾‹ï¼ŒæŸ¥çœ‹å…§æ”¿éƒ¨ä¸ŠæœŸçš„è³‡æ–™æœ‰ç„¡æ›´æ–°è‡³ 9/11
    - æœ‰
        - fetch 9/21 è³‡æ–™
        - å°‡ fetched çš„å…§å®¹å‚™ä»½è‡³ S3
        - parse content:
            - æ²’å•é¡Œçš„ records æ’å…¥ database
            - æœ‰å•é¡Œçš„å‰‡è½‰å›ä¸­æ–‡æ¬„ä½(äººå·¥æ ¡å°æ¯”è¼ƒæ–¹ä¾¿)ï¼Œå­˜æˆ json å‚™ä»½è‡³ S3
    - æ²’æœ‰ï¼Œreturn
- `APScheduler` ç™¼é€ JOB EXECUTED äº‹ä»¶
- listener åˆ¤æ–·æ˜¯å¦ç‚º crawl cronjob
    - æ˜¯ï¼Œå‘¼å« handler
        - scheduler æ–°å¢ check crawl çš„ job
    - å¦ï¼Œexit
- scheduler ç«‹å³åŸ·è¡Œ check crawl job
    - åˆ¤æ–·å‰›æ‰æŠ“åˆ°çš„æª”æ¡ˆæ—¥æœŸæ˜¯å¦ç‚º 9/21
        - æ˜¯ï¼Œ continue
        - å¦ï¼Œ raise exception
    - åˆ¤æ–·æ’å…¥ database çš„æœ€å¾Œä¸€ç­† created_at æ˜¯å¦ç‚ºç•¶æ—¥(ä»£è¡¨è³‡æ–™å­˜é€² database)
        - æ˜¯ï¼Œ continue
        - å¦ï¼Œ raise exception
- è‹¥ä¸Šä¸€æ­¥æ²’æœ‰ raise some exceptionï¼Œå‰‡ 9/21 çš„è³‡æ–™å·²æŠ“åˆ°
- è‹¥æœ‰ raise exceptionï¼Œ`APScheduler` ç™¼é€ JOB ERROR äº‹ä»¶
- listener åˆ¤æ–·æ˜¯å¦ç‚º check crawl çš„ job åŠç¬¦åˆçš„ exception type
    - æ˜¯ï¼Œå‘¼å« handler
        - scheduler æ–°å¢ 1 å°æ™‚ä¹‹å¾Œçš„ crawl cronjob
    - å¦ï¼Œexit

<!-- - Scheduler:

<p align="center">
<img src="./assets/scheduler.jpeg" alt="scheduler" width="300"/>
</p>

- Crawl process:

<p align="center">
<img src="./assets/crawl_job.jpeg" alt="crawl" width="300"/>
</p>

- Check process:

<p align="center">
<img src="./assets/check_job.jpeg" alt="check" width="300"/>
</p>

- Job executed listener

<p align="center">
<img src="./assets/job_executed_listener.jpeg" alt="executed_listener" width="300"/>
</p>

- Job error listener

<p align="center">
<img src="./assets/job_error_listener.jpeg" alt="error_listener" width="300"/>
</p> -->

## é è¨ˆå·¥ä½œ

- [ ] ç”¨ raw SQL statement æ–°å¢çµ±è¨ˆè¡¨ã€‚
    æ‰¾å‡ºæ–°åŒ—å¸‚æ·¡æ°´å€ä¸€å¹´å…§æ–°é–‹é å”®æ¡ˆï¼Œä¸¦çµ±è¨ˆå„æ¡ˆ 1ã€3 åŠ 6 å€‹æœˆæœŸé–“ï¼Œå…¶ 10 åªä»¥ä¸‹ã€20 åªã€30 åªåŠ 40 åªä»¥ä¸Šå„åˆ¥çš„å¹³å‡å–®åƒ¹åŠç¸½åƒ¹ã€‚
    é€™ä¸€æ®µæƒ³ç”¨ raw SQL statement åŸ·è¡Œï¼Œç›´æ¥ transform dataã€‚æœªä¾†ç•¶å…¶ä»–ç¸£å¸‚è³‡æ–™ä¹ŸåŠ é€²ä¾†æ™‚å› ç‚ºè³‡æ–™å¾ˆå¤šï¼Œå¯ä»¥è€ƒæ…®ç›´æ¥å¾ S3 copy é€² Redshift(data warehouse)ï¼Œé‚£æ™‚å€™å–®ç´” sql æŸ¥è©¢çš„æ•ˆç‡æœƒæ¯”è¼ƒé«˜ã€‚è€Œä¸”è‹¥å¯«åœ¨ python é€™é‚Šè™•ç†ï¼Œå¯èƒ½è³‡æ–™é‡å¤ªå¤§ï¼Œè¨˜æ†¶é«” load ä¸é€²ä¾†(è‡ªå·±çŒœæ¸¬)
    
    steps:
    1. æ¥åœ¨ check process ä¹‹å¾Œï¼Œç•¶ç¢ºèªæ²’å•é¡Œå‰‡é–‹å§‹é€™ä¸€æ®µ transform process
    2. models æ–°å¢ deal_statistics table
    3. ```sql
            INSERT INTO deal_statistics
            SELECT * FROM deals
            WHERE ...
            GROUP BY ...
        ```
    4. æ–°å¢ check deal_statistics process(æ˜¯å¦æœ‰å¿…è¦ï¼Ÿ)

- [ ] ä¸æ­£ç¢ºçš„ç´€éŒ„ï¼Œç¶“äººå·¥æ ¡æ­£å† insert into databaseã€‚
- [ ] å®šæœŸ mysql dumpï¼Œç›®å‰å¯ä»¥ç›´æ¥æŠ¹æ‰æ•´å¼µ tableï¼Œä½†æœªä¾†å¦‚æœ web application å¯ä»¥é–‹æ”¾ api å¾å‰ç«¯ update ä¸æ­£ç¢ºçš„è³‡è¨Šï¼Œå°±éœ€è¦å®šæœŸå‚™ä»½ã€‚

## ä¸€äº›æ€è€ƒ
é—œæ–¼è³‡æ–™åº«ã€æ’ç¨‹åŠæ¸¬è©¦è¨˜éŒ„åœ¨ [note.md](https://github.com/HMS24/pshouse_schedule/blob/master/assets/note.md)